## 3月28日 

目前运行文章数在200到300的作者，从数据库下载数据大概需要12小时，一共下载了七千多作者，运行算法1到两秒一个作者，大概3小时全部运行完。



## 3月27日 改进

在从数据库上读数据的时候最耗时的是轮询每一个作者的文章，而我们算法只需要文章数在某一个区间范围内的（这次是200到300），所以这个查询很耗时而且产出率不高。现在用get_paper_affiliations_num_by_author_name.py 这个代码，查询作者的文章数量并记录下来到author_papers_num.txt，只记录作者名字和对应的文章数量，首先速度**加快很多**，因为算法就不需要查询不需要的作者的数据，其次以后进行筛选的时候，不再需要重复筛选文章数目在一定范围内的作者。



## 3月22日-3月26日 改进

* 整体流程分为为三个部分
  1. 从数据库上得到数据并进行初步的cluster生成，dump在本地data文件夹中，代码在read_data_script中，（注：最耗时的一步，十二小时得到1000左右的作者，目前仍在服务器上/home/sjtuiiot/wb/name_dis_3_26/data运行）
  2. 算法通过dump load读取本地的数据，运行（速度很快 在PC上130秒运行了200个人名，运行算法几乎不耗时间）。最终把结果存到data文件夹对应的人名的文件夹内。（新代码在source文件夹内）
  3. 将结果更新数据库（未完成）
* 尝试将其中的dijstra算法改为floyd，更慢了一点，放弃。
* 韩家炜的人名相似算法的代码比较大，目前还在想办法融进，融入后应该能改进很多（未完成）
* 测试算法准确率的部分还未完成